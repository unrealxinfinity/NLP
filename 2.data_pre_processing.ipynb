{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the previous step in the pipeline: Importing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'source', 'reasoning'],\n",
      "        num_rows: 80000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'source', 'reasoning'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'source', 'reasoning'],\n",
      "        num_rows: 10200\n",
      "    })\n",
      "})\n",
      "                                                text            label  \\\n",
      "0  Your flight has been rescheduled for 10:00 AM ...          neutral   \n",
      "1  We're happy to accommodate your dietary prefer...           polite   \n",
      "2  Our vegetarian options are available on the me...          neutral   \n",
      "3  I understand your frustration with the recent ...  somewhat polite   \n",
      "4  I'll do my best to find a suitable replacement...  somewhat polite   \n",
      "\n",
      "                                  source  \\\n",
      "0  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "1  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "2  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "3  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "4  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "\n",
      "                                           reasoning  \n",
      "0  This text would be classified as \"neutral\" bec...  \n",
      "1  This text is polite because it expresses grati...  \n",
      "2  This text would be classified as \"neutral\" bec...  \n",
      "3  This text would be classified as \"somewhat pol...  \n",
      "4  This text would be classified as \"somewhat pol...  \n",
      "                                                text            label  \\\n",
      "0  I appreciate your interest in our vegetarian o...  somewhat polite   \n",
      "1  I understand you're concerned about the ski le...  somewhat polite   \n",
      "2  Our technical skills course will cover the ess...          neutral   \n",
      "3  Our buffet hours are from 11 AM to 9 PM. Pleas...          neutral   \n",
      "4  I'll look into your policy details and see wha...  somewhat polite   \n",
      "\n",
      "                                  source  \\\n",
      "0  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "1  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "2  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "3  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "4  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "\n",
      "                                           reasoning  \n",
      "0  This text is somewhat polite because it acknow...  \n",
      "1  This text would be classified as \"somewhat pol...  \n",
      "2  This text would be classified as \"neutral\" bec...  \n",
      "3  This text would be classified as \"neutral\" bec...  \n",
      "4  This text would be classified as \"somewhat pol...  \n",
      "                                                text     label  \\\n",
      "0  Are you seriously complaining about the tour n...  impolite   \n",
      "1  Are you seriously complaining about the delay ...  impolite   \n",
      "2  We're happy to accommodate your dietary needs....    polite   \n",
      "3  I appreciate your question about our fees. We ...    polite   \n",
      "4  We appreciate your trust in our banking servic...    polite   \n",
      "\n",
      "                                  source  \\\n",
      "0  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "1  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "2  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "3  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "4  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "\n",
      "                                           reasoning  \n",
      "0  This text is impolite because it uses a condes...  \n",
      "1  This text is impolite because it uses a confro...  \n",
      "2  This text is polite because it shows a willing...  \n",
      "3  This text is polite because it shows appreciat...  \n",
      "4  This text is polite because it expresses grati...  \n"
     ]
    }
   ],
   "source": [
    "%run ./1.importing_dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extracting text corpus**\n",
    "##### We have to extract the text from the documents in te dataset so we can use different representations to operate on.\n",
    "##### Note that this is an unclean version of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m unclean_corpus \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtraining_set\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[1;32m      3\u001b[0m     unclean_corpus\u001b[38;5;241m.\u001b[39mappend(training_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][i]);\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(unclean_corpus[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]);\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_set' is not defined"
     ]
    }
   ],
   "source": [
    "unclean_corpus = []\n",
    "for i in range(0, len(training_set[\"text\"])):\n",
    "    unclean_corpus.append(training_set['text'][i]);\n",
    "print(unclean_corpus[0:5]);\n",
    "\n",
    "\n",
    "unclean_corpus_test = []\n",
    "for i in range(0, len(test_set[\"text\"])):\n",
    "    unclean_corpus_test.append(test_set['text'][i]);\n",
    "print(unclean_corpus_test[0:5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cleaning the text corpus**\n",
    "##### Now we need to process the unclean text corpus, by performing actions such as:\n",
    "- ##### Removing punctuation;\n",
    "- ##### Lower case folding;\n",
    "- ##### Stemming (using PorterStemmer);\n",
    "- ##### Removing Stop Words (optional);\n",
    "##### For that effect we will import [regular expression](https://docs.python.org/3/library/re.html) library and [nltk](https://www.nltk.org/api/nltk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/magicojayz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/magicojayz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') \n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/magicojayz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/magicojayz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/magicojayz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight reschedule tomorrow please check airport web site update change', 'happy fit dietary taste vegetarian alternative carefully craft see delightful meet meal would like recommend serve meet require', 'vegetarian alternative available fare chef change serve case dietary require', 'see frustration recent tournament effect review stand see improve have', 'good find suited substitute item seem require know seem']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "synonym_cache = {}\n",
    "\n",
    "def freq_syn(word):\n",
    "    if word in synonym_cache:\n",
    "        return synonym_cache[word]\n",
    "    \n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        synonym_cache[word] = word \n",
    "        return word\n",
    "    \n",
    "    lemmas = []\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            lemmas.append((lemma.name(), lemma.count()))\n",
    "\n",
    "    if not lemmas:\n",
    "        synonym_cache[word] = word\n",
    "        return word\n",
    "\n",
    "    sorted_lemmas = sorted(lemmas, key=lambda x: x[1], reverse=True)\n",
    "    most_freq = sorted_lemmas[0][0].replace('_', ' ').lower()\n",
    "    synonym_cache[word] = most_freq\n",
    "    return most_freq\n",
    "\n",
    "clean_corpus = []\n",
    "\n",
    "for text in unclean_corpus:\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "    processed_words = []\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            synonym_word = freq_syn(word)\n",
    "            processed_words.append(synonym_word)\n",
    "\n",
    "    cleaned_text = ' '.join(processed_words)\n",
    "    clean_corpus.append(cleaned_text)\n",
    "\n",
    "    text = [ps.stem(word) for word in text.split() if not word in sw];\n",
    "    text = ' '.join(text);\n",
    "    clean_corpus.append(text);\n",
    "print(clean_corpus[0:5]);\n",
    "\n",
    "clean_corpus_test = []\n",
    "for i in range(0,len(unclean_corpus_test)):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', unclean_corpus_test[i]);\n",
    "    text = text.lower();\n",
    "\n",
    "    text = [ps.stem(word) for word in text.split() if not word in sw];\n",
    "    text = ' '.join(text);\n",
    "    clean_corpus_test.append(text);\n",
    "print(clean_corpus_test[0:5]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
