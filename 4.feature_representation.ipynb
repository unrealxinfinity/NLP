{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the previous step in the pipeline: Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./3.exploratory_data_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Representing the corpus in different models**\n",
    "##### Now that we have cleaned documents from our dataset, it's time to represent them in different textual representations so we can use them to train our model.\n",
    "#### **You might want to comment the blocks of code of the Space Representation Models section due to its sparity and they might be too inefficient due to the large dataset size.**\n",
    "#### **Some computers might not meet the hardware requirements to store these sparce vectors. So we limited the numer of max features each sparse vector representation had to 3000**. We chose this value because it seemed like a middle ground across different text representational models This has some implications in certain representational models as we will discuss next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000;\n",
    "feature_representations = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sparce representation models**\n",
    "#### We are working mainly with 3 sparse representational models:\n",
    "- #### **Bag of Words**;\n",
    "- #### **1-Hot vector**;\n",
    "- #### **Tf-Idf vector**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First representation model is **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer;\n",
    "import numpy as np;\n",
    "\n",
    "# print all the array content\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "#cv_orig = CountVectorizer(); commented for performance\n",
    "cv = CountVectorizer(max_features=max_features);\n",
    "\n",
    "#BoW_orig = cv_orig.fit_transform(clean_corpus).toarray(); commented for performance\n",
    "BoW = cv.fit_transform(clean_corpus).toarray();\n",
    "print(\"Cleaned text before the feature cut: 4981\"); #value obtained by analyzing the clean text before the cut\n",
    "print(\"Cleaned text after the cut:\", BoW.shape);\n",
    "\n",
    "feature_representations['BoW'] = (BoW, BoW_test);\n",
    "\n",
    "\n",
    "#print(cv.get_feature_names_out());\n",
    "\n",
    "#print(BoW[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We compared the representation of the unclean and clean version of the corpus as well, the unclean corpus would crash the kernel because the BoW generated would be too big for the memory according to its sparsity and number of features (>7000 approx.);\n",
    "##### As we can see, the cleaned text reduced the feature size (column number, y of the pair \"(x,y)\" of shape) by a significant amount because we reduced the words to their stem, standardized the capitalization and removed unecessary tokens like punctuation.\n",
    "##### This increases the model performance by eliminating unecessary repetitions.\n",
    "#### **Possible side effects of limiting the maximum number of features :**\n",
    "##### By **limiting the \"max_features\" to 3000**, despite incresing performance in terms of memory, we might be ignoring certain features that, despite having a low occurence in terms of term frequency across the documents, in specific documents of our corpus set (rows of annotated experiences of the trainees) might occur frequently, contributing to a better distinction of that specific document. This might degrade the performance of models trained using this representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_unclean = CountVectorizer(max_features=max_features);\n",
    "Bow_unclean = cv_unclean.fit_transform(unclean_corpus).toarray();\n",
    "print(\"Unclean text\",Bow_unclean.shape);\n",
    "print(\"Cleaned text\", BoW.shape);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also plot a **wordcloud** using the representation (although it only uses the text corpus to count and not the vectorizer) respectively for:\n",
    "- #### Unclean text;\n",
    "- #### Clean text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud;\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "\n",
    "wordcloud_unclean = wordcloud.WordCloud(width = 800, height = 800, background_color = 'white', stopwords = sw, min_font_size = 10).generate(\" \".join(unclean_corpus));\n",
    "plt.figure(figsize = (8, 8), facecolor = None);\n",
    "plt.imshow(wordcloud_unclean);\n",
    "plt.axis(\"off\");\n",
    "plt.tight_layout(pad = 0);\n",
    "print(\"Word Cloud for unclean corpus\");\n",
    "plt.show();\n",
    "\n",
    "wordcloud_clean = wordcloud.WordCloud(width = 800, height = 800, background_color = 'white', stopwords = sw, min_font_size = 10).generate(\" \".join(clean_corpus));\n",
    "plt.figure(figsize = (8, 8), facecolor = None);\n",
    "plt.imshow(wordcloud_clean);\n",
    "plt.axis(\"off\");\n",
    "plt.tight_layout(pad = 0);\n",
    "print(\"Word Cloud for clean corpus\");\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One noticeable difference is that some of the most common words like \"help\", \"question\" or \"appreciate\" were diminished, mainly because other words that were scattered in different forms in the unclean text were reduced to a common stem and a common standard so those words stand out more like \"provid\" \"hear\" and so on, because their counts increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second representation model is **1-Hot vector**\n",
    "##### This representation is basically Bag of Words but instead of focusing on the frequency of which the terms appear,it transforms each document into a binary vector where each dimension corresponds to a word from the vocabulary. If the word exists, a value of 1 is returned else a value of 0 indicating that the word does not exist. **We are only paying attention to term existence**\n",
    "#### **Possible side effects of limiting the maximum number of features :**\n",
    "##### Limiting the \"max_features\" in this representation might have a bigger impact than BoW, because BoW's term frequency might attenuate the decision made by, for instance, Naive Bayes, by having higher frequencies of the word of a certain class, balancing the cut made in less frequent words. On the other hand , 1-Hot focuses more on if the terms are indeed present in the vocabulary or not, meaning that a reduction in features from this representational model causes documents to lose vocabulary to distinguish one class from another, which is bad in this representation. However we could't find a workaround to maintain textual richness while decreasing memory used by the model so cutting features was a necessary step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "binary_vectorizer = CountVectorizer(binary=True,max_features=max_features)\n",
    "one_hot_clean = binary_vectorizer.fit_transform(clean_corpus)\n",
    "print(one_hot_clean.shape)\n",
    "\n",
    "one_hot_clean_test = binary_vectorizer.transform(clean_corpus_test);\n",
    "\n",
    "feature_representations['one_hot'] = (one_hot_clean, one_hot_clean_test);\n",
    "\n",
    "#print(one_hot_clean[0]);\n",
    "binary_vectorizer_unclean = CountVectorizer(binary=True,max_features=max_features)\n",
    "one_hot_unclean = binary_vectorizer.fit_transform(unclean_corpus)\n",
    "print(one_hot_unclean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third representation is **TF-IDF**\n",
    "##### This is a measure that takes into account the discriminative power of the words (repetitions of a word across documents/text or power of a word to distinguish the document content) from the vocabulary considering all the documents(our texts), by assigning a weight to each of the terms of the vocabulary.\n",
    "##### **TF** stands for Term Frequency and is the total frequency that a word appears considering all the documents;\n",
    "##### **DF** stands for Document Frequency and it measures the number of documents that have a certain word. The higher the more frequent is a word across all the documents (bad thing -> low discriminative power);\n",
    "##### **IDF** stands for Inverse Document Frequency and the inverse of the DF. Higher means rarer the word is across all the documents (good thing -> high discriminative power).\n",
    "##### **TF-IDF** is a measure that is the product of **TF** and **IDF**:\n",
    "- ##### Highest when t occurs many times within a small number of documents (A);\n",
    "- ##### Lower when the term occurs fewer times in a document, or occurs in many documents (B);\n",
    "- ##### Lowest when the term occurs in virtually all documents (C).\n",
    "### **Possible side effects limiting the maximum number of features:**\n",
    "##### For tf-idf representation, we know that it is ordered by the td-idf value. By cutting off the features that have the lowest tf-idf value means we are abandoning words with few discriminative power (words that are common across the documents with little differentiating power), which in this case is benefitial for this representational model in particular. As we can see in the tf-idf plot of the top words with most tf-idf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer;\n",
    "\n",
    "TfIdf_vectorizer_unclean = TfidfVectorizer(max_features = max_features);\n",
    "Tf_idf_unclean = TfIdf_vectorizer_unclean.fit_transform(unclean_corpus).toarray();\n",
    "Tf_idf_unclean_test = TfIdf_vectorizer_unclean.transform(unclean_corpus_test).toarray();\n",
    "features_unclean = TfIdf_vectorizer_unclean.get_feature_names_out();\n",
    "print(\"TfIdf unclean\",Tf_idf_unclean.shape)\n",
    "\n",
    "\n",
    "feature_representations['TfIdf'] = (Tf_idf_unclean, Tf_idf_unclean_test);\n",
    "\n",
    "TfIdf_vectorizer = TfidfVectorizer(max_features = max_features);\n",
    "Tf_Idf = TfIdf_vectorizer.fit_transform(clean_corpus).toarray();\n",
    "features_clean = TfIdf_vectorizer.get_feature_names_out();\n",
    "print(\"TfIdf clean\", Tf_Idf.shape);\n",
    "#print(Tf_Idf[0]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can plot a word cloud to see the distribution of the words for the clean and unclean corpus according to their weight and importance, with Tf-Idf representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to sum the weights of each unique word for the clean corpus\n",
    "word_count = dict()\n",
    "for i in range(0,len(clean_corpus)):\n",
    "    for feature in range(0,len(features_clean)):\n",
    "        word_count[features_clean[feature]] = word_count.get(features_clean[feature],0) + Tf_Idf[i][feature]\n",
    "\n",
    "wordcloud_tfidf_clean = wordcloud.WordCloud(width = 800, height = 800, background_color = 'white', min_font_size = 10).generate_from_frequencies(word_count);\n",
    "plt.figure(figsize = (8, 8), facecolor = None);\n",
    "plt.imshow(wordcloud_tfidf_clean);\n",
    "plt.axis(\"off\");\n",
    "plt.tight_layout(pad = 0);\n",
    "print(\"Word Cloud for clean corpus\");\n",
    "plt.show();\n",
    "\n",
    "#dict for unclean wordcount\n",
    "word_count_unclean = dict()\n",
    "for i in range(0,len(unclean_corpus)):\n",
    "    for feature in range(0,len(features_unclean)):\n",
    "        word_count_unclean[features_unclean[feature]] = word_count_unclean.get(features_unclean[feature],0) + Tf_idf_unclean[i][feature];\n",
    "\n",
    "wordcloud_tfidf_unclean = wordcloud.WordCloud(width = 800, height = 800, background_color = 'white', min_font_size = 10).generate_from_frequencies(word_count_unclean);\n",
    "plt.imshow(wordcloud_tfidf_unclean);\n",
    "plt.axis(\"off\");\n",
    "plt.tight_layout(pad = 0);\n",
    "print(\"Word Cloud for unclean corpus\");\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we predicted, the stop words took over the entire word cloud in the unclean dataset. These occur the most and give us the least information about a document fulliling the condition (C) presented in the block above.\n",
    "##### As for the clean version of the text corpus, words like \"like\" and \"help\" are most likely filling the condition (B), since they are often repeated and common in many documents in the dataset,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We opted not to use N-grams because the feature space it generates is too big and inefficient given our dataset, taking a toll on the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Beyond sparce representations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The most compatible representation models are dense vectors which reduce the dimensionality by a significant amount. The sparce vector models (all the previous models) waste memory space unecessarily due to their sparsity and due to the size of the dataset it is impossible to run those models in weaker machines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We considered 3 type of **Word Embeddings**:\n",
    "- ##### **Word2Vec**;\n",
    "- ##### **FastText**;\n",
    "- ##### **Doc2Vec**;\n",
    "##### Each of them have pros and cons:\n",
    "- ##### Word2Vec generates a high dimensionality vector taking word or phrases in the document just like Doc2Vec, but Doc2Vec handles larger text corpus (such as paragraphs or phrases) better than the Word2Vec;\n",
    "- ##### FastText handles better subword information, but we are working in terms of word and its semantics, so it would not help this project.\n",
    "##### So we opted using Word2Vec and Doc2Vec for comparison later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_embedding_unclean = Word2Vec(sentences = [text.split() for text in unclean_corpus], vector_size = 100, window = 5, min_count = 1, workers = 4);\n",
    "word2vec_embedding_clean = Word2Vec(sentences = [text.split() for text in clean_corpus], vector_size = 100, window = 5, min_count = 1, workers = 4);\n",
    "word2vec_embedding_clean_test = Word2Vec(sentences = [text.split() for text in clean_corpus_test], vector_size = 100, window = 5, min_count = 1, workers = 4);\n",
    "feature_representations['word2vec'] = (word2vec_embedding_clean, word2vec_embedding_clean_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_embedding_unclean.wv.most_similar('help'));\n",
    "print(word2vec_embedding_clean.wv.most_similar('help'));\n",
    "print(word2vec_embedding_unclean.wv[\"help\"]);\n",
    "print(word2vec_embedding_clean.wv[\"help\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents_clean = [TaggedDocument(_d, [str(i)]) for i, _d in enumerate(clean_corpus)];\n",
    "documents_clean_test = [TaggedDocument(_d, [str(i)]) for i, _d in enumerate(clean_corpus_test)];\n",
    "documents_unclean = [TaggedDocument(_d, [str(i)]) for i, _d in enumerate(unclean_corpus)];\n",
    "doc2vec_embedding_clean = Doc2Vec(documents_clean, vector_size=5, window=2, min_count=1, workers=4);\n",
    "doc2vec_embedding_clean_test = Doc2Vec(documents_clean_test, vector_size=5, window=2, min_count=1, workers=4);\n",
    "doc2vec_embedding_unclean = Doc2Vec(documents_unclean, vector_size=5, window=2, min_count=1, workers=4);\n",
    "feature_representations['doc2vec'] = (doc2vec_embedding_clean, doc2vec_embedding_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors of the first document in the clean and unclean corpus\n",
    "print(doc2vec_embedding_clean.dv[0]);\n",
    "print(doc2vec_embedding_unclean.dv[0]);\n",
    "# Finding most similar documents to the first document in the clean corpus\n",
    "print(doc2vec_embedding_clean.dv.most_similar([doc2vec_embedding_clean.dv[0]]))\n",
    "# Finding most similar documents to the first document in the unclean corpus\n",
    "print(doc2vec_embedding_unclean.dv.most_similar([doc2vec_embedding_unclean.dv[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary, the following representational models are what we are going to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "#print(BoW.shape);\n",
    "# TfIdf\n",
    "#print(Tf_Idf.shape);\n",
    "# One Hot\n",
    "#print(one_hot_clean.shape);\n",
    "# Word2Vec vocab mapping with vectors\n",
    "#print(word2vec_embedding_clean.wv);\n",
    "# Doc2Vec document mapping with vectors\n",
    "#print(doc2vec_embedding_clean.dv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training**\n",
    "\n",
    "##### As we have our feature representations, we can start to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total Memory: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available Memory: {mem.available / 1e9:.2f} GB\")\n",
    "print(f\"Used Memory: {mem.used / 1e9:.2f} GB ({mem.percent}%)\")\n",
    "\n",
    "y_train = training_set['label']\n",
    "y_test = test_set['label']\n",
    "# Define Models\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"MLP\": MLPClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "# Train and Evaluate Models for Each Feature Representation\n",
    "final_results = {}\n",
    "\n",
    "for feature_name, (X_train, X_test) in feature_representations.items():  # Extract (x_train, x_test)\n",
    "    print(f\"\\nEvaluating Feature Representation: {feature_name}\")\n",
    "    results = {}\n",
    "\n",
    "    # Standardize if required (for models that need it)\n",
    "    if feature_name in [\"word2vec_embedding_clean\", \"doc2vec_embedding_clean\", \"Tf_Idf\", \"BoW\"]:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)  # Train on training set\n",
    "        y_pred = model.predict(X_test)  # Predict on test set\n",
    "        accuracy = accuracy_score(y_test, y_pred)  # Evaluate\n",
    "        results[model_name] = accuracy\n",
    "        print(f\"{model_name}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "    final_results[feature_name] = results  # Store results for this feature representation\n",
    "\n",
    "# Print Overall Results\n",
    "print(\"\\n===== Model Performance Across Feature Representations =====\")\n",
    "for feature, results in final_results.items():\n",
    "    print(f\"\\nFeature Representation: {feature}\")\n",
    "    for model, acc in results.items():\n",
    "        print(f\"{model}: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
