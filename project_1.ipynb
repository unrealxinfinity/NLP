{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing First Assignment\n",
    "#### This is the notebook for the first assignment about the dataset **\"Polite Guard\"**. The objective of this work is to come up with a pipeline that builds a robust and good model for text classification\n",
    "#### The following dependencies are needed:\n",
    "`\n",
    "pip install datasets pandas nltk scikit-learn wordcloud matplotlib gensim\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing the dataset**\n",
    "##### The first step is to import the dataset we are using, directly from the library provided by Hugging Face. The original dataset already split test and training data, as well as validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/not_real_fu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'source', 'reasoning'],\n",
      "        num_rows: 80000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'source', 'reasoning'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'source', 'reasoning'],\n",
      "        num_rows: 10200\n",
      "    })\n",
      "})\n",
      "                                                text            label  \\\n",
      "0  Your flight has been rescheduled for 10:00 AM ...          neutral   \n",
      "1  We're happy to accommodate your dietary prefer...           polite   \n",
      "2  Our vegetarian options are available on the me...          neutral   \n",
      "3  I understand your frustration with the recent ...  somewhat polite   \n",
      "4  I'll do my best to find a suitable replacement...  somewhat polite   \n",
      "\n",
      "                                  source  \\\n",
      "0  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "1  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "2  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "3  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "4  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "\n",
      "                                           reasoning  \n",
      "0  This text would be classified as \"neutral\" bec...  \n",
      "1  This text is polite because it expresses grati...  \n",
      "2  This text would be classified as \"neutral\" bec...  \n",
      "3  This text would be classified as \"somewhat pol...  \n",
      "4  This text would be classified as \"somewhat pol...  \n",
      "                                                text            label  \\\n",
      "0  I appreciate your interest in our vegetarian o...  somewhat polite   \n",
      "1  I understand you're concerned about the ski le...  somewhat polite   \n",
      "2  Our technical skills course will cover the ess...          neutral   \n",
      "3  Our buffet hours are from 11 AM to 9 PM. Pleas...          neutral   \n",
      "4  I'll look into your policy details and see wha...  somewhat polite   \n",
      "\n",
      "                                  source  \\\n",
      "0  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "1  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "2  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "3  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "4  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "\n",
      "                                           reasoning  \n",
      "0  This text is somewhat polite because it acknow...  \n",
      "1  This text would be classified as \"somewhat pol...  \n",
      "2  This text would be classified as \"neutral\" bec...  \n",
      "3  This text would be classified as \"neutral\" bec...  \n",
      "4  This text would be classified as \"somewhat pol...  \n",
      "                                                text     label  \\\n",
      "0  Are you seriously complaining about the tour n...  impolite   \n",
      "1  Are you seriously complaining about the delay ...  impolite   \n",
      "2  We're happy to accommodate your dietary needs....    polite   \n",
      "3  I appreciate your question about our fees. We ...    polite   \n",
      "4  We appreciate your trust in our banking servic...    polite   \n",
      "\n",
      "                                  source  \\\n",
      "0  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "1  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "2  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "3  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "4  meta-llama/Meta-Llama-3.1-8B-Instruct   \n",
      "\n",
      "                                           reasoning  \n",
      "0  This text is impolite because it uses a condes...  \n",
      "1  This text is impolite because it uses a confro...  \n",
      "2  This text is polite because it shows a willing...  \n",
      "3  This text is polite because it shows appreciat...  \n",
      "4  This text is polite because it expresses grati...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd;\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Intel/polite-guard\");\n",
    "print(dataset);\n",
    "\n",
    "training_set = pd.DataFrame.from_dict(dataset['train']);\n",
    "test_set = pd.DataFrame.from_dict(dataset['test']);\n",
    "validation_set = pd.DataFrame.from_dict(dataset['validation']);\n",
    "\n",
    "print(training_set.head());\\\n",
    "print(test_set.head());\n",
    "print(validation_set.head());\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The text corpuses are mainly about different types of language in terms of formality, the label to be classified is if the corpus is a polite or impolite. **There are 4 classes which are \"impolite\" , \"neutral\" , \"somewhat polite\" and \"polite\"**\n",
    "##### There are four features of the dataset which are:\n",
    "- ##### **text** - actual text corpus;\n",
    "- ##### **label** - class label to identify text formality;\n",
    "- ##### **reasoning** - why it was labeled as that class;\n",
    "- ##### **source** - source of the text;\n",
    "##### The majority of the dataset were **result of prompting using different synthesis techniques**, including only 200 annotated real-life examples from corporate training.\n",
    "##### The prompting techniques used: \n",
    "- ##### 50,000 samples generated using Few-Shot prompting\n",
    "- ##### 50,000 samples generated using Chain-of-Thought (CoT) prompting\n",
    "- ##### 200 annotated samples from corporate trainings; \n",
    "##### The test/train/validation split was done as follows:\n",
    "- ##### **80k rows for training**;\n",
    "- ##### **20k rows for testing**;\n",
    "- ##### **remaining rows for validation (200 annotated samples)**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extracting text corpus**\n",
    "##### We have to extract the text from the documents in te dataset so we can use different representations to operate on.\n",
    "##### Note that this is an unclean version of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Your flight has been rescheduled for 10:00 AM tomorrow. Please check the airport's website for any updates or changes.\", \"We're happy to accommodate your dietary preferences. Our vegetarian options are carefully crafted to ensure a delicious and satisfying meal. Would you like me to recommend some dishes that fit your needs?\", 'Our vegetarian options are available on the menu, and our chef can modify any dish to suit your dietary needs.', \"I understand your frustration with the recent tournament results, and I'll review the standings to see what we can do to improve your experience.\", \"I'll do my best to find a suitable replacement for the item you're looking for, but I need to know more about what you're looking for.\"]\n"
     ]
    }
   ],
   "source": [
    "unclean_corpus = []\n",
    "for i in range(0, len(training_set[\"text\"])):\n",
    "    unclean_corpus.append(training_set['text'][i]);\n",
    "print(unclean_corpus[0:5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cleaning the text corpus**\n",
    "##### Now we need to process the unclean text corpus, by performing actions such as:\n",
    "- ##### Removing punctuation;\n",
    "- ##### Lower case folding;\n",
    "- ##### Stemming (using PorterStemmer);\n",
    "- ##### Removing Stop Words (optional);\n",
    "##### For that effect we will import [regular expression](https://docs.python.org/3/library/re.html) library and [nltk](https://www.nltk.org/api/nltk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/not_real_fu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight reschedul tomorrow pleas check airport websit updat chang', 'happi accommod dietari prefer vegetarian option care craft ensur delici satisfi meal would like recommend dish fit need', 'vegetarian option avail menu chef modifi dish suit dietari need', 'understand frustrat recent tournament result review stand see improv experi', 'best find suitabl replac item look need know look']\n"
     ]
    }
   ],
   "source": [
    "import nltk;\n",
    "nltk.download('stopwords')\n",
    "import re;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem.porter import PorterStemmer;\n",
    "\n",
    "ps = PorterStemmer();\n",
    "sw = stopwords.words('english');\n",
    "clean_corpus = []\n",
    "for i in range(0,len(unclean_corpus)):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', unclean_corpus[i]);\n",
    "    text = text.lower();\n",
    "\n",
    "    text = [ps.stem(word) for word in text.split() if not word in sw];\n",
    "    text = ' '.join(text);\n",
    "    clean_corpus.append(text);\n",
    "print(clean_corpus[0:5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Updating the Dataframe with unclean corpus clean corpus for comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[\"clean_text\"] = clean_corpus;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Representing the corpus in different models**\n",
    "##### Now that we have cleaned documents from our dataset, it's time to represent them in different textual representations so we can use them to train our model.\n",
    "#### **You might want to comment the blocks of code of the Space Representation Models section due to its sparity and they might be too inefficient due to the large dataset size.**\n",
    "#### **Some computers might not meet the hardware requirements to store these sparce vectors. So comment if you think your computer can't handle it for the feature space of < 9000 columns and dataset of 80k rows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sparce representation models**\n",
    "#### We are working mainly with 3 sparse representational models:\n",
    "- #### **Bag of Words**;\n",
    "- #### **1-Hot vector**;\n",
    "- #### **Tf-Idf vector**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First representation model is **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text (80000, 4981)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer;\n",
    "import numpy as np;\n",
    "\n",
    "# print all the array content\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "cv = CountVectorizer();\n",
    "\n",
    "BoW = cv.fit_transform(clean_corpus).toarray();\n",
    "print(\"Cleaned text\", BoW.shape);\n",
    "\n",
    "\n",
    "#print(cv.get_feature_names_out());\n",
    "\n",
    "#print(BoW[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We compared the representation of the unclean and clean version of the corpus as well\n",
    "##### As we can see, the cleaned text reduced the feature size (column number, y of the pair \"(x,y)\" of shape) by a significant amount because we reduced the words to their stem, standardized the capitalization and removed unecessary tokens like punctuation.\n",
    "##### This increases the model performance by eliminating unecessary repetitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cv_unclean = CountVectorizer();\\nBow_unclean = cv_unclean.fit_transform(unclean_corpus).toarray();\\nprint(\"Unclean text\",Bow_unclean.shape);\\nprint(\"Cleaned text\", BoW.shape);\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''cv_unclean = CountVectorizer();\n",
    "Bow_unclean = cv_unclean.fit_transform(unclean_corpus).toarray();\n",
    "print(\"Unclean text\",Bow_unclean.shape);\n",
    "print(\"Cleaned text\", BoW.shape);\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also plot a **wordcloud** using the representation (although it only uses the text corpus to count and not the vectorizer) respectively for:\n",
    "- #### Unclean text;\n",
    "- #### Clean text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwordcloud_unclean = wordcloud.WordCloud(width = 800, height = 800, background_color = \\'white\\', stopwords = sw, min_font_size = 10).generate(\" \".join(unclean_corpus));\\nplt.figure(figsize = (8, 8), facecolor = None);\\nplt.imshow(wordcloud_unclean);\\nplt.axis(\"off\");\\nplt.tight_layout(pad = 0);\\nprint(\"Word Cloud for unclean corpus\");\\nplt.show();\\n\\nwordcloud_clean = wordcloud.WordCloud(width = 800, height = 800, background_color = \\'white\\', stopwords = sw, min_font_size = 10).generate(\" \".join(clean_corpus));\\nplt.figure(figsize = (8, 8), facecolor = None);\\nplt.imshow(wordcloud_clean);\\nplt.axis(\"off\");\\nplt.tight_layout(pad = 0);\\nprint(\"Word Cloud for clean corpus\");\\nplt.show();'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wordcloud;\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "'''\n",
    "wordcloud_unclean = wordcloud.WordCloud(width = 800, height = 800, background_color = 'white', stopwords = sw, min_font_size = 10).generate(\" \".join(unclean_corpus));\n",
    "plt.figure(figsize = (8, 8), facecolor = None);\n",
    "plt.imshow(wordcloud_unclean);\n",
    "plt.axis(\"off\");\n",
    "plt.tight_layout(pad = 0);\n",
    "print(\"Word Cloud for unclean corpus\");\n",
    "plt.show();\n",
    "\n",
    "wordcloud_clean = wordcloud.WordCloud(width = 800, height = 800, background_color = 'white', stopwords = sw, min_font_size = 10).generate(\" \".join(clean_corpus));\n",
    "plt.figure(figsize = (8, 8), facecolor = None);\n",
    "plt.imshow(wordcloud_clean);\n",
    "plt.axis(\"off\");\n",
    "plt.tight_layout(pad = 0);\n",
    "print(\"Word Cloud for clean corpus\");\n",
    "plt.show();'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One noticeable difference is that some of the most common words like \"help\", \"question\" or \"appreciate\" were diminished, mainly because other words that were scattered in different forms in the unclean text were reduced to a common stem and a common standard so those words stand out more like \"provid\" \"hear\" and so on, because their counts increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second representation model is **1-Hot vector**\n",
    "##### This representation is basically Bag of Words but without counts, only a binary number indicating if the word exists or not accross the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 4981)\n",
      "(80000, 8532)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "binary_vectorizer = CountVectorizer(binary=True);\n",
    "one_hot_clean = binary_vectorizer.fit_transform(clean_corpus);\n",
    "print(one_hot_clean.shape);\n",
    "\n",
    "#print(one_hot_clean[0]);\n",
    "binary_vectorizer_unclean = CountVectorizer(binary=True);\n",
    "one_hot_unclean = binary_vectorizer.fit_transform(unclean_corpus);\n",
    "print(one_hot_unclean.shape);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third representation is **TF-IDF**\n",
    "##### This is a measure that takes into account the discriminative power of the words (repetitions of a word accross documents/text or power of a word to distinguish the document content) from the vocabulary considering all the documents(our texts), by assigning a weight to each of the terms of the vocabulary.\n",
    "##### **TF** stands for Term Frequency and is the total frequency that a word appears considering all the documents;\n",
    "##### **DF** stands for Document Frequency and it measures the number of documents that have a certain word. The higher the more frequent is a word accross all the documents (bad thing -> low discriminative power);\n",
    "##### **IDF** stands for Inverse Document Frequency and the inverse of the DF. Higher means rarer the word is accross all the documents (good thing -> high discriminative power).\n",
    "##### **TF-IDF** is a measure that is the product of **TF** and **IDF**:\n",
    "- ##### Highest when t occurs many times within a small number of documents (A);\n",
    "- ##### Lower when the term occurs fewer times in a document, or occurs in many documents (B);\n",
    "- ##### Lowest when the term occurs in virtually all documents (C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfIdf unclean (80000, 8532)\n",
      "TfIdf clean (80000, 4981)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer;\n",
    "\n",
    "TfIdf_vectorizer_unclean = TfidfVectorizer();\n",
    "Tf_idf_unclean = TfIdf_vectorizer_unclean.fit_transform(unclean_corpus).toarray();\n",
    "features_unclean = TfIdf_vectorizer_unclean.get_feature_names_out();\n",
    "print(\"TfIdf unclean\",Tf_idf_unclean.shape)\n",
    "\n",
    "TfIdf_vectorizer = TfidfVectorizer();\n",
    "Tf_Idf = TfIdf_vectorizer.fit_transform(clean_corpus).toarray();\n",
    "features_clean = TfIdf_vectorizer.get_feature_names_out();\n",
    "print(\"TfIdf clean\", Tf_Idf.shape);\n",
    "#print(Tf_Idf[0]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can plot a word cloud to see the distribution of the words for the clean and unclean corpus according to their weight and importance, with Tf-Idf representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_count = dict()\\nfor i in range(0,len(clean_corpus)):\\n    for feature in range(0,len(features_clean)):\\n        word_count[features_clean[feature]] = word_count.get(features_clean[feature],0) + Tf_Idf[i][feature]\\n\\nwordcloud_tfidf_clean = wordcloud.WordCloud(width = 800, height = 800, background_color = \\'white\\', min_font_size = 10).generate_from_frequencies(word_count);\\nplt.figure(figsize = (8, 8), facecolor = None);\\nplt.imshow(wordcloud_tfidf_clean);\\nplt.axis(\"off\");\\nplt.tight_layout(pad = 0);\\nprint(\"Word Cloud for clean corpus\");\\nplt.show();\\n\\n#dict for unclean wordcount\\nword_count_unclean = dict()\\nfor i in range(0,len(unclean_corpus)):\\n    for feature in range(0,len(features_unclean)):\\n        word_count_unclean[features_unclean[feature]] = word_count_unclean.get(features_unclean[feature],0) + Tf_idf_unclean[i][feature];\\n\\nwordcloud_tfidf_unclean = wordcloud.WordCloud(width = 800, height = 800, background_color = \\'white\\', min_font_size = 10).generate_from_frequencies(word_count_unclean);\\nplt.imshow(wordcloud_tfidf_unclean);\\nplt.axis(\"off\");\\nplt.tight_layout(pad = 0);\\nprint(\"Word Cloud for unclean corpus\");\\nplt.show();'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary to sum the weights of each unique word for the clean corpus\n",
    "'''word_count = dict()\n",
    "for i in range(0,len(clean_corpus)):\n",
    "    for feature in range(0,len(features_clean)):\n",
    "        word_count[features_clean[feature]] = word_count.get(features_clean[feature],0) + Tf_Idf[i][feature]\n",
    "\n",
    "wordcloud_tfidf_clean = wordcloud.WordCloud(width = 800, height = 800, background_color = 'white', min_font_size = 10).generate_from_frequencies(word_count);\n",
    "plt.figure(figsize = (8, 8), facecolor = None);\n",
    "plt.imshow(wordcloud_tfidf_clean);\n",
    "plt.axis(\"off\");\n",
    "plt.tight_layout(pad = 0);\n",
    "print(\"Word Cloud for clean corpus\");\n",
    "plt.show();\n",
    "\n",
    "#dict for unclean wordcount\n",
    "word_count_unclean = dict()\n",
    "for i in range(0,len(unclean_corpus)):\n",
    "    for feature in range(0,len(features_unclean)):\n",
    "        word_count_unclean[features_unclean[feature]] = word_count_unclean.get(features_unclean[feature],0) + Tf_idf_unclean[i][feature];\n",
    "\n",
    "wordcloud_tfidf_unclean = wordcloud.WordCloud(width = 800, height = 800, background_color = 'white', min_font_size = 10).generate_from_frequencies(word_count_unclean);\n",
    "plt.imshow(wordcloud_tfidf_unclean);\n",
    "plt.axis(\"off\");\n",
    "plt.tight_layout(pad = 0);\n",
    "print(\"Word Cloud for unclean corpus\");\n",
    "plt.show();'''\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we predicted, the stop words took over the entire word cloud in the unclean dataset. These occur the most and give us the least information about a document fulliling the condition (C) presented in the block above.\n",
    "##### As for the clean version of the text corpus, words like \"like\" and \"help\" are most likely filling the condition (B), since they are often repeated and common in many documents in the dataset,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We opted not to use N-grams because the feature space it generates is too big and inefficient given our dataset, taking a toll on the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Beyond sparce representations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The most compatible representation models are dense vectors which reduce the dimensionality by a significant amount. The sparce vector models (all the previous models) waste memory space unecessarily due to their sparsity and due to the size of the dataset it is impossible to run those models in weaker machines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We considered 3 type of **Word Embeddings**:\n",
    "- ##### **Word2Vec**;\n",
    "- ##### **FastText**;\n",
    "- ##### **Doc2Vec**;\n",
    "##### Each of them have pros and cons:\n",
    "- ##### Word2Vec generates a high dimensionality vector taking word or phrases in the document just like Doc2Vec, but Doc2Vec handles larger text corpus (such as paragraphs or phrases) better than the Word2Vec;\n",
    "- ##### FastText handles better subword information, but we are working in terms of word and its semantics, so it would not help this project.\n",
    "##### So we opted using Word2Vec and Doc2Vec for comparison later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('assist', 0.5985382795333862), ('happy', 0.4253496825695038), ('us!', 0.4059656858444214), ('help.', 0.4044547975063324), ('shopping', 0.3524917662143707), ('skills!', 0.34916985034942627), ('equip', 0.3451002836227417), ('frustration', 0.3441314995288849), ('answer', 0.34399035573005676), ('specialists', 0.33235788345336914)]\n",
      "[('assist', 0.655402421951294), ('happi', 0.4399434030056), ('regard', 0.4313318133354187), ('appreci', 0.35334745049476624), ('provid', 0.3486132025718689), ('suitabl', 0.34127140045166016), ('concern', 0.332386314868927), ('misunderstand', 0.32823318243026733), ('thank', 0.32348790764808655), ('certainli', 0.322783887386322)]\n",
      "Word2Vec<vocab=4989, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_embedding_unclean = Word2Vec(sentences = [text.split() for text in unclean_corpus], vector_size = 100, window = 5, min_count = 1, workers = 4);\n",
    "word2vec_embedding_clean = Word2Vec(sentences = [text.split() for text in clean_corpus], vector_size = 100, window = 5, min_count = 1, workers = 4);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('assist', 0.5985382795333862), ('happy', 0.4253496825695038), ('us!', 0.4059656858444214), ('help.', 0.4044547975063324), ('shopping', 0.3524917662143707), ('skills!', 0.34916985034942627), ('equip', 0.3451002836227417), ('frustration', 0.3441314995288849), ('answer', 0.34399035573005676), ('specialists', 0.33235788345336914)]\n",
      "[('assist', 0.655402421951294), ('happi', 0.4399434030056), ('regard', 0.4313318133354187), ('appreci', 0.35334745049476624), ('provid', 0.3486132025718689), ('suitabl', 0.34127140045166016), ('concern', 0.332386314868927), ('misunderstand', 0.32823318243026733), ('thank', 0.32348790764808655), ('certainli', 0.322783887386322)]\n",
      "[ 1.7538501   2.4359643   2.075145    6.5213313   0.33731833  3.710494\n",
      "  2.4197502   2.3675892  -0.7035968  -2.6525035  -2.1895292  -1.2027342\n",
      "  3.8873377   0.24061668  0.35402876 -0.8865496   0.47483388  1.6164935\n",
      "  1.2088014   0.4597224  -1.6718328   1.3218437   0.8569037   2.8103912\n",
      "  4.327201    3.5933511  -3.5309832   0.12094443  0.701977    1.1179824\n",
      " -1.9395592  -2.6298394  -0.783976   -2.8262048   1.2313446   0.64047325\n",
      "  4.8911176   0.90884316 -1.3466227   3.2126725   2.3467705  -3.8742657\n",
      " -0.49914142 -1.4830052   2.41435    -1.1039106   1.6776416  -1.6392606\n",
      " -1.2508688  -0.38544336  1.2865107   2.2685802  -0.6431853  -0.36833698\n",
      "  4.473053   -1.3315796  -1.6164082  -1.6838524  -0.408609   -1.3737783\n",
      " -1.9044427   0.3660222   1.2033893   0.76562023  0.6852142  -2.7292964\n",
      " -0.7299689  -2.623065   -3.5733817  -1.9887338  -0.953309    1.6473287\n",
      " -0.0996814   1.080856   -2.2969933   0.22334561  0.5738305  -0.14069237\n",
      "  1.893949    5.647306   -1.1834548   1.7777145   3.1524794  -1.9084541\n",
      " -1.314348    0.44265926  0.93719494 -2.6061985  -2.3923228  -2.998623\n",
      "  1.855984   -3.5160666   0.37931868 -1.9714259   0.33037052 -2.354402\n",
      "  2.4421647  -1.1030504  -0.6148292   3.077416  ]\n",
      "[ 0.37198174  0.66910625 -0.17551596  1.8832631  -0.53098714 -0.17222218\n",
      " -0.3510416  -0.06395864 -1.068103   -0.9501925   1.6719172   2.8427448\n",
      " -0.14541245  1.2019814   1.6565182   3.1722536   0.2416072   0.7381966\n",
      " -1.6230768  -0.56904083 -0.523401    0.07442061  1.6616173  -0.12231014\n",
      " -1.025796   -2.1017165  -1.5972188   0.21008375 -0.3323676   0.11908881\n",
      "  1.6032543  -0.605812    1.5500915  -1.7655488   0.7104717  -0.7450857\n",
      "  0.02011053  1.2312     -0.20743547  1.2919983   0.6656072   1.0910674\n",
      "  0.1003404   1.3336536   0.25435218 -0.9965284   1.1811582   1.907059\n",
      " -0.4031848  -0.07644022  0.5148397  -1.1289417  -0.6945882   0.9485234\n",
      "  1.152568    1.2328885  -0.32823232 -0.5535358  -0.8550343  -2.6842587\n",
      " -0.09024028  0.77030575 -0.2368134   0.3609626   0.03471356 -0.22919518\n",
      "  1.5062679  -2.1447515  -2.0128775  -0.03391405  1.1646286  -0.1606749\n",
      "  1.142061    1.988954    0.89848226  1.2016426   0.38743174  0.87770444\n",
      " -1.8459268   1.5191106  -0.43320447 -1.3685375   0.30427712 -0.7885427\n",
      "  0.84926397 -1.4703673  -1.424656   -4.0319567   0.17403446  1.8116288\n",
      " -1.6168967   2.2387676   0.55752146 -1.4825393   0.32538995  0.37750235\n",
      " -1.2929752   2.0918603  -1.7992667   1.5166768 ]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_embedding_unclean.wv.most_similar('help'));\n",
    "print(word2vec_embedding_clean.wv.most_similar('help'));\n",
    "print(word2vec_embedding_unclean.wv[\"help\"]);\n",
    "print(word2vec_embedding_clean.wv[\"help\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents_clean = [TaggedDocument(_d, [str(i)]) for i, _d in enumerate(clean_corpus)];\n",
    "documents_unclean = [TaggedDocument(_d, [str(i)]) for i, _d in enumerate(unclean_corpus)];\n",
    "doc2vec_embedding_clean = Doc2Vec(documents_clean, vector_size=5, window=2, min_count=1, workers=4);\n",
    "doc2vec_embedding_unclean = Doc2Vec(documents_unclean, vector_size=5, window=2, min_count=1, workers=4);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09117847 -0.17477754  0.00054884  0.19604598  0.04935723]\n",
      "[-0.11124151 -0.11866955 -0.39550716  0.5509691  -0.19465415]\n",
      "[('0', 1.0), ('2668', 0.9994635581970215), ('3868', 0.9990739226341248), ('5521', 0.9988207221031189), ('76529', 0.9988175630569458), ('66069', 0.9987872242927551), ('14095', 0.9985340237617493), ('73649', 0.9981640577316284), ('19306', 0.9979440569877625), ('36119', 0.9978556036949158)]\n",
      "[('0', 0.9999999403953552), ('36829', 0.9990250468254089), ('15922', 0.9989022612571716), ('45547', 0.9988818764686584), ('32645', 0.9984160661697388), ('29271', 0.9983428120613098), ('19578', 0.9981349110603333), ('39596', 0.997815728187561), ('19047', 0.9977831244468689), ('21903', 0.9976970553398132)]\n"
     ]
    }
   ],
   "source": [
    "# Vectors of the first document in the clean and unclean corpus\n",
    "print(doc2vec_embedding_clean.dv[0]);\n",
    "print(doc2vec_embedding_unclean.dv[0]);\n",
    "# Finding most similar documents to the first document in the clean corpus\n",
    "print(doc2vec_embedding_clean.dv.most_similar([doc2vec_embedding_clean.dv[0]]))\n",
    "# Finding most similar documents to the first document in the unclean corpus\n",
    "print(doc2vec_embedding_unclean.dv.most_similar([doc2vec_embedding_unclean.dv[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary, the following representational models are what we are going to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 4981)\n",
      "(80000, 4981)\n",
      "(80000, 4981)\n",
      "Word2Vec<vocab=4989, vector_size=100, alpha=0.025>\n",
      "Doc2Vec<dm/m,d5,n5,w2,s0.001,t4>\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "print(BoW.shape);\n",
    "# TfIdf\n",
    "print(Tf_Idf.shape);\n",
    "# One Hot\n",
    "print(one_hot_clean.shape);\n",
    "# Word2Vec\n",
    "print(word2vec_embedding_clean);\n",
    "# Doc2Vec\n",
    "print(doc2vec_embedding_clean);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
